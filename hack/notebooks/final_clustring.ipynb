{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import OPTICS\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\source\\cp_video\\env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle(\"whisper.pkl\")\n",
    "model_name = 'clip-ViT-B-16'\n",
    "st_model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.read_pickle(r\"C:\\Users\\user\\source\\cp_video\\data\\frames.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crop_person_photos(whisper_df_: pd.DataFrame, time_frames: List[Tuple[int, np.ndarray]]):\n",
    "    \"\"\"Получить кропнутые фотки людей и их секунда\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    whisper_df_ : pd.DataFrame\n",
    "        wisper после добавления всех колонок\n",
    "    time_frames :  List[Tuple[int, np.ndarray]]\n",
    "        Фреймы и время \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        frames - массив обрезанных фото\n",
    "        frame_numbers - время фото\n",
    "\n",
    "    \"\"\"\n",
    "    whisper_df = whisper_df_.copy()\n",
    "\n",
    "    if all(col in whisper_df.columns for col in [\"objects\", \"objects_boxes\"]):\n",
    "        whisper_df[\"objects\"] = whisper_df[\"objects\"].map(lambda x: x[0] if len(x) > 0 else [])\n",
    "        whisper_df[\"objects_boxes\"] = whisper_df[\"objects_boxes\"].map(lambda x: x[0] if len(x) > 0 else [])\n",
    "    else:\n",
    "        raise ValueError(\"Required columns 'objects' and 'objects_boxes' are not present in whisper_df.\")\n",
    "    \n",
    "    cropped_frames_dict = {\n",
    "        \"frames\": [],  # массив для обрезанных фреймов\n",
    "        \"frame_numbers\": []  # массив для номеров фреймов\n",
    "    }\n",
    "\n",
    "    for index, row in whisper_df.iterrows():\n",
    "        frame_number = int(row['start'])\n",
    "        if frame_number >= len(time_frames):\n",
    "            break\n",
    "        frame = time_frames[frame_number][1]  # извлекаем текущий фрейм\n",
    "        objects = row['objects']\n",
    "        boxes = row['objects_boxes']\n",
    "        if len(objects) == 0:\n",
    "            continue\n",
    "\n",
    "        for obj, box in zip(objects, boxes):\n",
    "            if obj == 'person':\n",
    "                x_min, y_min, x_max, y_max = box\n",
    "                cropped_frame = frame[int(y_min):int(y_max), int(x_min):int(x_max)]\n",
    "                \n",
    "                # Добавляем обрезанный фрейм и номер фрейма в словарь\n",
    "                cropped_frames_dict[\"frames\"].append(cropped_frame)\n",
    "                cropped_frames_dict[\"frame_numbers\"].append(index)\n",
    "    return cropped_frames_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_img(image_array, model=st_model):\n",
    "    if isinstance(image_array, np.ndarray):\n",
    "        img = Image.fromarray(image_array)\n",
    "    else:\n",
    "        raise ValueError(\"Input must be a NumPy array.\")\n",
    "    return model.encode(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_crop_embedd(frames):\n",
    "#     embedding = []\n",
    "#     for emb in tqdm(frames):\n",
    "#         embed_frame = vectorize_img(emb)\n",
    "#         embedding.append(embed_frame)\n",
    "#     return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frames = np.load(\"person_embedding.npy\")\n",
    "def get_crop_embedd():\n",
    "    frames = np.load(\"person_embedding.npy\")\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaler(emb: pd.Series):\n",
    "    '''\n",
    "    StandartScaler для ембеддингов\n",
    "    '''\n",
    "    embeddings = np.vstack(emb.values)\n",
    "    scaler = StandardScaler()\n",
    "    scale_emb = scaler.fit_transform(embeddings)\n",
    "    return scale_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optics_clustering(embeddings: np.ndarray, \n",
    "                      min_samples: int = 20,\n",
    "                      xi: float = 0.01,\n",
    "                      min_cluster_size: float = 0.01):\n",
    "    \"\"\"\n",
    "    Выполняет кластеризацию с использованием алгоритма OPTICS\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embeddings : np.ndarray\n",
    "        Массив эмбеддингов, где каждая строка представляет собой вектор признаков для одного объекта.\n",
    "        \n",
    "    min_samples : int, по умолчанию 20\n",
    "        Минимальное количество образцов в группе, необходимое для создания кластера. \n",
    "        Используется для определения плотности.\n",
    "\n",
    "    xi : float, по умолчанию 0.01\n",
    "        Параметр, определяющий, как сильно отделяются кластеры друг от друга. \n",
    "        Чем больше значение, тем менее агрессивная отделка.\n",
    "\n",
    "    min_cluster_size : float, по умолчанию 0.01\n",
    "        Минимальный размер кластера как доля от общего числа точек. \n",
    "        Если размер кластера меньше этого значения, он не будет считаться кластером.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Массив меток кластеров, где каждая метка соответствует строке в массиве `embeddings`. \n",
    "        Значение -1 обозначает шум (выбросы).\n",
    "    \"\"\"\n",
    "    optics = OPTICS(min_samples=min_samples, xi=xi, min_cluster_size=min_cluster_size, metric='cosine')\n",
    "    clusters = optics.fit_predict(embeddings)\n",
    "    return clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_example(stats_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Находит ближайшие объекты до центра кластера для каждого кластера на основе их эмбеддингов.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    stats_df : pd.DataFrame\n",
    "        DataFrame, содержащий эмбеддинги и метки кластеров, \n",
    "        полученные из функции кластеризации.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Словарь, где ключами являются метки кластеров, \n",
    "        а значениями - индексы объектов, ближайших к центру кластера.\n",
    "    \"\"\"\n",
    "    df_all_labels = np.copy(np.vstack(stats_df[\"embeddings\"].values))\n",
    "    cluster = np.array(stats_df[\"cluster\"])\n",
    "    unique_labels = set(list(stats_df[\"cluster\"]))\n",
    "    unique_labels.discard(-1)\n",
    "\n",
    "    closest_objects = {}\n",
    "    for label in unique_labels:\n",
    "        cluster_indices = np.where(cluster == label)[0]\n",
    "        cluster_objects = df_all_labels[cluster_indices]\n",
    "        cluster_center = np.mean(cluster_objects, axis=0)\n",
    "        distances = pairwise_distances(cluster_objects, cluster_center.reshape(1, -1), metric='cosine').flatten()\n",
    "        sorted_indices = np.argsort(distances)\n",
    "        n = 5\n",
    "        closest_n_objects = cluster_indices[sorted_indices[:n]]\n",
    "        closest_objects[label] = closest_n_objects\n",
    "    return closest_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_main_photo(path: str, closest_objects: dict, cropped_frames_dict: dict):\n",
    "    \"\"\"\n",
    "    Сохраняет фотографии в заданную директорию на основе индексов ближайших объектов.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Путь к директории, в которую будут сохранены фотографии.\n",
    "    \n",
    "    closest_objects : dict\n",
    "        Словарь, где ключами являются метки кластеров, \n",
    "        а значениями - индексы объектов, которые нужно сохранить.\n",
    "\n",
    "    cropped_frames_dict : dict\n",
    "        Словарь, содержащий обрезанные фотографии, \n",
    "        доступные по индексу.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[str]\n",
    "        Функция сохраняет фотографии и возвращает пути до них.\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    paths = []\n",
    "    for label, indices in closest_objects.items():\n",
    "        for index in indices:\n",
    "            # Извлечение обрезанного изображения по индексу\n",
    "            # print(index, cropped_frames_dict[\"frame_numbers\"])\n",
    "            if index in cropped_frames_dict[\"frame_numbers\"]:  # Убедимся, что индекс существует\n",
    "                image = cropped_frames_dict[\"frames\"][index]  # Получаем изображение\n",
    "                if image.shape[0] > 300 and image.shape[1] > 270:\n",
    "                    # Формируем имя файла\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                    base_filename = f'cluster_{label}_object_{index}.jpg'\n",
    "                    filename = os.path.join(path, base_filename)\n",
    "                    \n",
    "                    # Проверка существования файла и добавление суффикса _{k} при необходимости\n",
    "                    k = 0\n",
    "                    while os.path.exists(filename):\n",
    "                        k += 1\n",
    "                        filename = os.path.join(path, f'cluster_{label}_object_{index}_{k}.jpg')\n",
    "                    cv2.imwrite(filename, image)  # Сохраняем изображение\n",
    "                    paths.append(filename)\n",
    "    return paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering(whisper_df_: pd.DataFrame, time_frames: List[Tuple[int, np.ndarray]], path: str):\n",
    "    \"\"\"\n",
    "    Выполняет кластеризацию обрезанных фотографий людей на основе их эмбеддингов. Сохраняет фотографии\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    whisper_df_ : pd.DataFrame\n",
    "        wisper после добавления всех колонок\n",
    "    time_frames :  List[Tuple[int, np.ndarray]]\n",
    "        Фреймы и время \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Функция сохраняет фотографии в указанной директории. И возвращает пути\n",
    "    \"\"\"\n",
    "    cropped_frames_dict = get_crop_person_photos(whisper_df_, time_frames)\n",
    "    # embeddings = get_crop_embedd(cropped_frames_dict[\"frames\"])\n",
    "    embeddings = get_crop_embedd()\n",
    "\n",
    "    embeddings = np.array(embeddings)\n",
    "    stats_df = pd.DataFrame({\"embeddings\": list(embeddings)})\n",
    "    embeddings = scaler(stats_df[\"embeddings\"])\n",
    "\n",
    "    clusters = optics_clustering(embeddings)\n",
    "    stats_df[\"cluster\"] = list(clusters)\n",
    "    closest_objects = get_best_example(stats_df)\n",
    "    paths = save_main_photo(path, closest_objects, cropped_frames_dict)\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\user\\\\source\\\\cp_video\\\\cut video\\\\images\\\\cluster_1_object_64.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\source\\\\cp_video\\\\cut video\\\\images\\\\cluster_2_object_42.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\source\\\\cp_video\\\\cut video\\\\images\\\\cluster_3_object_356.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\source\\\\cp_video\\\\cut video\\\\images\\\\cluster_4_object_364.jpg',\n",
       " 'C:\\\\Users\\\\user\\\\source\\\\cp_video\\\\cut video\\\\images\\\\cluster_9_object_335.jpg']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustering(df, frame, r\"C:\\Users\\user\\source\\cp_video\\cut video\\images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
